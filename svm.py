# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HHiVH_CoIO9IQaq-MkKmrIc_6gKX6uj
"""

#install imports

import pandas as pd
import numpy as np
import kagglehub
import nltk
import re
import bz2
import os
import math
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
uploaded = files.upload()

nltk.download('punkt')

# Replace with your actual path if needed
train_file = 'new_train.ft.txt'  # Upload this file to your notebook if using Colab

reviews = []
labels = []
max_reviews = 100000

with open(train_file, 'r', encoding='ISO-8859-1') as f:
    for i, line in enumerate(f):
        if i >= max_reviews:
            break
        parts = line.strip().split(' ', 1)
        if len(parts) != 2:
            continue
        label_raw, text = parts
        if label_raw == "__label__1":
            sentiment = "Negative"
        elif label_raw == "__label__2":
            sentiment = "Positive"
        else:
            continue
        labels.append(sentiment)
        reviews.append(text)

df = pd.DataFrame({'Review': reviews, 'Sentiment': labels})
print(f"Loaded {len(df)} reviews successfully!")
print(df['Sentiment'].value_counts())

# Train/test split
X_train_texts, X_test_texts, y_train, y_test = train_test_split(
    df['Review'], df['Sentiment'], test_size=0.3, random_state=42
)

# Vectorize raw data
vectorizer = CountVectorizer(ngram_range=(1,2))  # unigrams + bigrams
X_train = vectorizer.fit_transform(X_train_texts)
X_test = vectorizer.transform(X_test_texts)

# train model

svm_model = LinearSVC(max_iter=10000)
svm_model.fit(X_train, y_train)

# prediction method

y_pred = svm_model.predict(X_test)

# model evaluation

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# print wrong predictions
wrong_indexes = np.where(y_pred != y_test)[0]

for i in wrong_indexes[:10]:  # first 10 mistakes
    print(f"Review: {X_test_texts.iloc[i]}")
    print(f"True label: {y_test.iloc[i]}")
    print(f"Predicted label: {y_pred[i]}")
    print("-" * 50)

feature_names = vectorizer.get_feature_names_out()
coefficients = svm_model.coef_[0]

# analyzing n grams in wrong review
review_text = """Prehistory - Schmistory: Intellectually and artistically roughly on a par with the Flintstones. Despite the numerous five star reviews it has received here this risible prehistoric bonkbuster is utter dross, a hopelessly 'eighties modernist' take on the past diplaying minimal knowledge of human nature and the way new ideas and societies really evolve.Of course good fairy tales can be entertaining for children and adults alike and are an esential part of our cultural heritage. Not this one though"""

# vectorize the review
review_vector = vectorizer.transform([review_text])

# get nonzero features (ngrams)
nonzero_indexes = review_vector.nonzero()[1]

# ngrams and their weights
ngrams_and_weights = []

for idx in nonzero_indexes:
    ngram = feature_names[idx]
    weight = coefficients[idx]
    ngrams_and_weights.append((ngram, weight))

# sort the ngrams by influence (weight)
ngrams_and_weights_sorted = sorted(ngrams_and_weights, key=lambda x: abs(x[1]), reverse=True)

# print
print("Top N-grams contributing to the prediction:\n")
for ngram, weight in ngrams_and_weights_sorted[:10]:  # top 10
    direction = "Positive" if weight > 0 else "Negative"
    print(f"N-gram: {ngram:30} | Weight: {weight:+.4f} ({direction})")